{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWS Sagemaker Image Classification of Dog Breed \n",
    "\n",
    "This notebook shows how to do image classification with a pre-trained model in AWS Sagemaker.\n",
    "We use a pretrained convolutional neural network with 50 layers (resnet50) in Pytorch Vision. The CNN is improved by using residual learning that shortcuts connections by skipping layers.\n",
    "\n",
    "The model is fine-tuned to our image data set, consisting of dog images labelled by breed. \n",
    "This is done using a hyper parameter job to train it in Sagemaker. \n",
    "Finally, we enable profiling and debugging with hooks to track the progress of our model training. Ultimately, we obtain a supervised image classifier that can tell the breed of dogs.\n",
    "\n",
    "Debugging helps troubleshoot the training of our neural network (e.g., vanishing/exploding gradients, bad weight initialization, saturated activation, overfitting, class imbalance, loss).\n",
    "\n",
    "Profiling helps create a report while training that tracks the progress of our model training.\n",
    "Finally, profiling can help measure GPU, CPU, and memory utilization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: smdebug in /opt/conda/lib/python3.7/site-packages (1.0.12)\n",
      "Requirement already satisfied: numpy>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from smdebug) (1.20.3)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from smdebug) (20.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from smdebug) (3.19.1)\n",
      "Requirement already satisfied: boto3>=1.10.32 in /opt/conda/lib/python3.7/site-packages (from smdebug) (1.20.23)\n",
      "Requirement already satisfied: pyinstrument==3.4.2 in /opt/conda/lib/python3.7/site-packages (from smdebug) (3.4.2)\n",
      "Requirement already satisfied: pyinstrument-cext>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from pyinstrument==3.4.2->smdebug) (0.2.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.10.32->smdebug) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.10.32->smdebug) (1.23.23)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.10.32->smdebug) (0.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->smdebug) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging->smdebug) (1.14.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.10.32->smdebug) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.23->boto3>=1.10.32->smdebug) (2.8.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install packages \n",
    "# We will need the smdebug package|\n",
    "!pip install smdebug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages that we need\n",
    "# For instance, we will Boto3 and Sagemaker\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region us-west-1 and S3 Bucket: sagemaker-us-west-1-625155689245\n"
     ]
    }
   ],
   "source": [
    "# Set up role, sagemaker, and s3 bucket \n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "print(f\"Region {region} and S3 Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are using an image data set of dogs that are labelled by their breed, which can be 133 different kinds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to download and unzip data (NB: only run once) \n",
    "!wget https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip\n",
    "!unzip dogImages.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch and upload the data to AWS S3 (NB: only run once)\n",
    "image_data_folder_path =\"dogImages\"\n",
    "s3_file_path = session.upload_data(path=image_data_folder_path, bucket=bucket, key_prefix=image_data_folder_path)\n",
    "print(f\"S3 File Path: {s3_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_file_path = \"s3://sagemaker-us-west-1-625155689245/dogImages\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We will finetune a pretrained model with hyperparameter tuning. \n",
    "We need to tune at minimum two hyperparameters.\n",
    "Optionally, we can explain why we pick those hyperparameters and ranges.\n",
    "\n",
    "To do finetuning (i.e., transfer learning), we will add a fully connected neural network layer on top of the pretrained model as the output layer, which can perform classification of 133 breeds. \n",
    "\n",
    "\n",
    "We use the Adam optimizer (with adaptive learning rate) from Pytorch and the following hyperparameters:\n",
    "- Learning rate: 0.000001 (10^-6) to 1.0 (10^0) \n",
    "- Epsilon: 1 to 3 \n",
    "- Weight decay: 0 to 0.1 (1e-1) \n",
    "\n",
    "The learning rate controls the step size at which we move in the direction towards the minimum of a loss function during each training iteration. Epsilon controls the threshold at which we automatically discard redundant layers that produce responses smaller than this threshold: [epsilon-ResNet](https://arxiv.org/abs/1804.01661). Namely, epsilon controls the tradeoff between accuracy and network size and is typically in the 1-3 range. The weight decay is a regularizing term controlling the weights by penalizing/shrinking them during backprogation to avoid overfitting. It is usually set between 0 and 0.1: [ML Mastery](https://machinelearningmastery.com/how-to-reduce-overfitting-in-deep-learning-with-weight-regularization/). \n",
    "\n",
    "The ranges are picked randomly through trial and error experimentation and rough guidelines. \n",
    "\n",
    "Typical values for a neural network with standardized inputs (or inputs mapped to (0, 1) interval) are less than 1 and greater than 10^-6: [ML Mastery](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/). \n",
    "\n",
    "**Note:** We will need to use the `hpo.py` script to perform hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare HP ranges, metrics etc.\n",
    "from sagemaker.tuner import (\n",
    "    CategoricalParameter,\n",
    "    ContinuousParameter,\n",
    "    HyperparameterTuner\n",
    ")\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "    \"lr\": ContinuousParameter(0.000001, 1.0),\n",
    "    \"eps\": ContinuousParameter(1, 3),\n",
    "    \"weight_decay\": ContinuousParameter(0, 0.1),\n",
    "    \"batch_size\": CategoricalParameter([64, 128]), # Picked range similar to exercises for speedup\n",
    "}\n",
    "\n",
    "objective_metric_name = \"avg_test_loss\"\n",
    "objective_type = \"Minimize\" \n",
    "metric_definitions = [{\"Name\": objective_metric_name, \"Regex\": \"Test set: Average loss: ([0-9\\\\.]+)\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create estimator for our HPs\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "estimator = PyTorch(\n",
    "    entry_point = \"hpo.py\", # Hyper parameter entry point for training \n",
    "    base_job_name = \"sagemaker-dog-breed-image-classification-hyperparameter-tuning\",\n",
    "    role = role,\n",
    "    instance_count = 1, \n",
    "    instance_type = \"ml.g4dn.xlarge\", # if ml.g4dn.xlarge (4 CPU, 16GB RAM, GPU) not working try ml.m5.xlarge  \n",
    "    py_version = \"py36\", \n",
    "    framework_version = \"1.8\"\n",
    ") \n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    metric_definitions, # Extracts average loss from logs with regex\n",
    "    max_jobs=1, # Max train jobs \n",
    "    max_parallel_jobs=1, # Max parallel training jobs to start\n",
    "    objective_type=objective_type,\n",
    "    early_stopping_type=\"Auto\" # Early stopping may happen \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................................................................................................................................!\n"
     ]
    }
   ],
   "source": [
    "# Fit your HP Tuner\n",
    "tuner.fit({\"training\": s3_file_path }, wait=True) # Remember to include your data channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best estimators and the best HPs\n",
    "\n",
    "best_estimator = tuner.best_estimator()\n",
    "\n",
    "#Get the hyperparameters of the best trained model\n",
    "best_estimator.hyperparameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters : \n",
      " {'batch_size': 128, 'eps': '2.8673436008071853', 'lr': '0.0017412062280646723', 'weight_decay': '0.005611348748266832'}\n"
     ]
    }
   ],
   "source": [
    "best_hyperparameters={'batch_size': int(best_estimator.hyperparameters()['batch_size'].replace('\"', \"\")),\n",
    "                      'eps': best_estimator.hyperparameters()['eps'],\n",
    "                      'lr': best_estimator.hyperparameters()['lr'],\n",
    "                      'weight_decay': best_estimator.hyperparameters()['weight_decay'],}\n",
    "print(f\"Best hyperparameters : \\n {best_hyperparameters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hyperparameters = {'batch_size': 128, 'eps': '2.8673436008071853', 'lr': '0.0017412062280646723', 'weight_decay': '0.005611348748266832'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Profiling and Debugging\n",
    "We use the best hyperparameters to create and finetune a new model\n",
    "\n",
    "**Note:** We will need to use the `train_model.py` script to perform model profiling and debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up debugger and profiler rules and configs\n",
    "from sagemaker.debugger import (\n",
    "    Rule,\n",
    "    rule_configs, \n",
    "    ProfilerRule,\n",
    "    DebuggerHookConfig,\n",
    "    CollectionConfig,\n",
    "    ProfilerConfig,\n",
    "    FrameworkProfile\n",
    ")\n",
    "\n",
    "rules = [\n",
    "    Rule.sagemaker(rule_configs.vanishing_gradient()),\n",
    "    Rule.sagemaker(rule_configs.overfit()),\n",
    "    Rule.sagemaker(rule_configs.overtraining()),\n",
    "    Rule.sagemaker(rule_configs.poor_weight_initialization()),\n",
    "    ProfilerRule.sagemaker(rule_configs.ProfilerReport()),\n",
    "]\n",
    "\n",
    "profiler_config = ProfilerConfig(\n",
    "    system_monitor_interval_millis=500, framework_profile_params=FrameworkProfile(num_steps=10)\n",
    ")\n",
    "\n",
    "collection_configs=[CollectionConfig(name=\"CrossEntropyLoss_output_0\",parameters={\n",
    "    \"include_regex\": \"CrossEntropyLoss_output_0\", \"train.save_interval\": \"10\",\"eval.save_interval\": \"1\"})]\n",
    "\n",
    "debugger_config=DebuggerHookConfig( collection_configs=collection_configs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-08 16:26:17 Starting - Starting the training job...\n",
      "2022-04-08 16:26:46 Starting - Preparing the instances for trainingVanishingGradient: InProgress\n",
      "Overfit: InProgress\n",
      "Overtraining: InProgress\n",
      "PoorWeightInitialization: InProgress\n",
      "ProfilerReport: InProgress\n",
      "......\n",
      "2022-04-08 16:27:46 Downloading - Downloading input data............\n",
      "2022-04-08 16:29:47 Training - Downloading the training image...\n",
      "2022-04-08 16:30:14 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-04-08 16:30:16,952 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-04-08 16:30:16,980 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-04-08 16:30:16,987 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-04-08 16:30:17,362 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 128,\n",
      "        \"eps\": \"2.8673436008071853\",\n",
      "        \"lr\": \"0.0017412062280646723\",\n",
      "        \"weight_decay\": \"0.005611348748266832\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"pytorch-training-2022-04-08-16-26-16-613\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-1-625155689245/pytorch-training-2022-04-08-16-26-16-613/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_model\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_model.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":128,\"eps\":\"2.8673436008071853\",\"lr\":\"0.0017412062280646723\",\"weight_decay\":\"0.005611348748266832\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_model.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_model\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-1-625155689245/pytorch-training-2022-04-08-16-26-16-613/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":128,\"eps\":\"2.8673436008071853\",\"lr\":\"0.0017412062280646723\",\"weight_decay\":\"0.005611348748266832\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"pytorch-training-2022-04-08-16-26-16-613\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-1-625155689245/pytorch-training-2022-04-08-16-26-16-613/source/sourcedir.tar.gz\",\"module_name\":\"train_model\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_model.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"128\",\"--eps\",\"2.8673436008071853\",\"--lr\",\"0.0017412062280646723\",\"--weight_decay\",\"0.005611348748266832\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=128\u001b[0m\n",
      "\u001b[34mSM_HP_EPS=2.8673436008071853\u001b[0m\n",
      "\u001b[34mSM_HP_LR=0.0017412062280646723\u001b[0m\n",
      "\u001b[34mSM_HP_WEIGHT_DECAY=0.005611348748266832\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train_model.py --batch_size 128 --eps 2.8673436008071853 --lr 0.0017412062280646723 --weight_decay 0.005611348748266832\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:19.107 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:19.462 algo-1:27 INFO profiler_config_parser.py:102] Using config at /opt/ml/input/config/profilerconfig.json.\u001b[0m\n",
      "\u001b[34mRunning on Device cuda:0\u001b[0m\n",
      "\u001b[34mHyperparameters : LR: 0.0017412062280646723,  Eps: 2.8673436008071853, Weight-decay: 0.005611348748266832, Batch Size: 128, Epoch: 2\u001b[0m\n",
      "\u001b[34mData Dir Path: /opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mModel Dir  Path: /opt/ml/model\u001b[0m\n",
      "\u001b[34mOutput Dir  Path: /opt/ml/output/data\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:25.832 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:25.833 algo-1:27 INFO hook.py:199] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:25.835 algo-1:27 INFO hook.py:253] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:25.835 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:25.862 algo-1:27 INFO hook.py:584] name:fc.0.weight count_params:524288\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:25.862 algo-1:27 INFO hook.py:584] name:fc.0.bias count_params:256\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:25.863 algo-1:27 INFO hook.py:584] name:fc.2.weight count_params:34048\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:25.863 algo-1:27 INFO hook.py:584] name:fc.2.bias count_params:133\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:25.864 algo-1:27 INFO hook.py:586] Total Trainable Params: 558725\u001b[0m\n",
      "\u001b[34mEpoch 1 - Starting Training phase.\u001b[0m\n",
      "\u001b[34mEpoch: 1 - Training Model on Complete Training Dataset!\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:28.271 algo-1:27 INFO hook.py:413] Monitoring the collections: relu_input, gradients, CrossEntropyLoss_output_0, losses\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:28.273 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/prestepzero-*-start-1649435419462355.8_train-0-stepstart-1649435428273215.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:28.291 algo-1:27 INFO hook.py:476] Hook is writing from the hook with pid: 27\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:53.716 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-0-stepstart-1649435428288927.2_train-0-forwardpassend-1649435453716653.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:30:55.915 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-0-forwardpassend-1649435453720083.8_train-1-stepstart-1649435455914809.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:00.242 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-1-stepstart-1649435455920733.0_train-1-forwardpassend-1649435460239554.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:02.572 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-1-forwardpassend-1649435460246137.8_train-2-stepstart-1649435462572517.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:06.616 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-2-stepstart-1649435462576112.8_train-2-forwardpassend-1649435466616140.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:09.057 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-2-forwardpassend-1649435466618040.8_train-3-stepstart-1649435469056474.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:13.475 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-3-stepstart-1649435469060943.2_train-3-forwardpassend-1649435473474884.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:16.003 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-3-forwardpassend-1649435473477515.2_train-4-stepstart-1649435476002687.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:20.404 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-4-stepstart-1649435476007249.5_train-4-forwardpassend-1649435480404013.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:23.431 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-4-forwardpassend-1649435480405984.2_train-5-stepstart-1649435483431308.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:27.743 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-5-stepstart-1649435483438972.8_train-5-forwardpassend-1649435487743132.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:29.965 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-5-forwardpassend-1649435487745370.8_train-6-stepstart-1649435489964671.2/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:34.445 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-6-stepstart-1649435489967935.5_train-6-forwardpassend-1649435494444682.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:36.882 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-6-forwardpassend-1649435494446564.0_train-7-stepstart-1649435496881711.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:41.391 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-7-stepstart-1649435496885199.2_train-7-forwardpassend-1649435501390891.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:43.234 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-7-forwardpassend-1649435501393005.2_train-8-stepstart-1649435503233453.5/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:47.895 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-8-stepstart-1649435503238587.5_train-8-forwardpassend-1649435507895485.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:49.801 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-8-forwardpassend-1649435507897429.0_train-9-stepstart-1649435509800933.8/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:54.325 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-9-stepstart-1649435509804165.0_train-9-forwardpassend-1649435514325096.0/python_stats.\u001b[0m\n",
      "\u001b[34m[2022-04-08 16:31:56.257 algo-1:27 INFO python_profiler.py:182] Dumping cProfile stats to /opt/ml/output/profiler/framework/pytorch/cprofile/27-algo-1/train-9-forwardpassend-1649435514327398.2_train-10-stepstart-1649435516256520.8/python_stats.\u001b[0m\n",
      "\u001b[34mTrain set: Average loss: 4.8877, Accuracy: 86/6680 (1%)\u001b[0m\n",
      "\u001b[34mEpoch 1 - Starting Testing phase.\u001b[0m\n",
      "\u001b[34mEpoch: 1 - Testing Model on Complete Testing Dataset!\u001b[0m\n",
      "\u001b[34mTest set: Average loss: 4.8867, Accuracy: 12/836 (1%)\u001b[0m\n",
      "\u001b[34mEpoch 2 - Starting Training phase.\u001b[0m\n",
      "\u001b[34mEpoch: 2 - Training Model on Complete Training Dataset!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Create and fit an estimator\n",
    "estimator = PyTorch(\n",
    "    entry_point=\"train_model.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g4dn.xlarge\",\n",
    "    role=role,\n",
    "    framework_version=\"1.6\", # 1.6 supports smdebug lib\n",
    "    py_version=\"py36\",\n",
    "    hyperparameters=best_hyperparameters,\n",
    "    profiler_config=profiler_config, # include profiler hook\n",
    "    debugger_hook_config=debugger_config, # include debugger hook\n",
    "    rules=rules\n",
    ")\n",
    "\n",
    "estimator.fit({'train' : s3_file_path },wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch job name, client and description for plotting\n",
    "job_name = estimator.latest_training_job.name\n",
    "client = estimator.sagemaker_session.sagemaker_client\n",
    "description = client.describe_training_job(TrainingJobName=estimator.latest_training_job.name)\n",
    "print(f\"Jobname: {job_name}\")\n",
    "print(f\"Client: {client}\")\n",
    "print(f\"Description: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trail object to query for tensors in given training job, based on smdebug artifact path \n",
    "from smdebug.trials import create_trial\n",
    "from smdebug.core.modes import ModeKeys\n",
    "trial = create_trial(estimator.latest_job_debugger_artifacts_path())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a debugging output\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "\n",
    "# helper to get data from tensors\n",
    "def get_data_from_tensors(trial, tname, mode):\n",
    "    tensor = trial.tensor(tname)\n",
    "    steps = tensor.steps(mode=mode)\n",
    "    values = []\n",
    "    for s in steps:\n",
    "        values.append(tensor.value(s, mode=mode))\n",
    "    return steps, values\n",
    "\n",
    "# helper to plot tensor values over time \n",
    "def plot_tensor(trial, tensor_name):\n",
    "\n",
    "    training_steps, training_values = get_data_from_tensors(trial, tensor_name, mode=ModeKeys.TRAIN)\n",
    "    print(\"loaded training data\")\n",
    "    test_steps, test_values = get_data_from_tensors(trial, tensor_name, mode=ModeKeys.EVAL)\n",
    "    print(\"loaded test data\")\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    host = host_subplot(112)\n",
    "    par = host.twiny()\n",
    "    host.set_xlabel(\"Steps - training\")\n",
    "    par.set_xlabel(\"Steps - testing\")\n",
    "    host.set_ylabel(tensor_name)\n",
    "\n",
    "    (p1,) = host.plot(training_steps, training_values, label=tensor_name)\n",
    "    print(\"Completed training plot\")\n",
    "    (p2,) = par.plot(test_steps, test_values, label=\"val_\" + tensor_name)\n",
    "    print(\"Completed testing plot\")\n",
    "    leg = plt.legend()\n",
    "\n",
    "    host.xaxis.get_label().set_color(p1.get_color())\n",
    "    leg.texts[0].set_color(p1.get_color())\n",
    "    par.xaxis.get_label().set_color(p2.get_color())\n",
    "    leg.texts[1].set_color(p2.get_color())\n",
    "    plt.ylabel(tensor_name)\n",
    "    plt.show()\n",
    "plot_tensor(trial, \"CrossEntropyLoss_output_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is there some anomalous behaviour in your debugging output? If so, what is the error and how will you fix it?  \n",
    "- The loss is not smooth but very spiky. \n",
    "- Using batches of data with shuffling might help stabilize it or trying a different neural network architecture setup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the profiler output\n",
    "profiler_report_path = estimator.output_path + estimator.latest_training_job.job_name + \"/rule-output\"\n",
    "print(f\"Profiler report location: {profiler_report_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 ls {profiler_report_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! aws s3 cp {profiler_report_path} ./ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# get auto generated profiler report folder name\n",
    "profiler_report_name = [\n",
    "    rule[\"RuleConfigurationName\"]\n",
    "    for rule in estimator.latest_training_job.rule_job_summary()\n",
    "    if \"Profiler\" in rule[\"RuleConfigurationName\"]\n",
    "][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.HTML(filename=profiler_report_name + \"/profiler-output/profiler-report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip profiler report for submission\n",
    "import shutil\n",
    "shutil.make_archive(\"./profiler_repot.zip\", \"zip\", \"ProfilerReport\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deploying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy our model to an endpoint\n",
    "# Add your deployment configuration like instance type and number of instances\n",
    "predictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a prediction on the endpoint\n",
    "# Our code to load and preprocess image to send to endpoint for prediction\n",
    "test_dir = \"./dogImages/test/\"\n",
    "test_img = \"011.Australian_cattle_dog/Australian_cattle_dog_00727.jpg\"\n",
    "expected_label = 11 \n",
    "test_file_path = os.path.join(test_dir, test_img)\n",
    "with open(test_file_path , \"rb\") as f:\n",
    "    image = f.read()\n",
    "    print(\"Testing image\")\n",
    "    display(Image.open(io.BytesIO(image)))\n",
    "    print(f\"Expected dog breed label: {expected_label}\")\n",
    "    response = predictor.predict(image, initial_args={\"ContentType\": \"image/jpeg\"})\n",
    "    print(f\"Response: {response}\")\n",
    "    predicted_dog_breed = np.argmax(response, 1) + 1 # Index starts from 0 as prediction is 0-indexed\n",
    "    print(f\"Response/Inference for the above image is : {predicted_dog_breed}\")\n",
    "\n",
    "response = predictor.predict(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to shutdown/delete your endpoint once your work is done\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-1:742091327244:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
